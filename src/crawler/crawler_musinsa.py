#%%
import os
import time
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from typing import Optional, Union
import pandas as pd

#%%
# ì €ì¥ í´ë”
SAVE_DIR = "./musinsa_images"
os.makedirs(SAVE_DIR, exist_ok=True)

# ëª©í‘œ ê°œìˆ˜ (ì¹´í…Œê³ ë¦¬ë³„)
TARGET_COUNT = 200
SCROLL_PAUSE_TIME = 1.5

# ì¹´í…Œê³ ë¦¬ ì •ë³´
categories = {
    "ìƒì˜" : "001",
    "ê°€ë°©": "004",
    "ì•„ìš°í„°": "002", 
    "ë°”ì§€": "003",
    "íŒ¨ì…˜ì†Œí’ˆ": "101",
    "ìŠ¤í¬ì¸ /ë ˆì €": "017",
    "ì‹ ë°œ": "103"
}

sex = "gf=M"
sort = "sortCode=SALE_ONE_WEEK_COUNT"

def crawl_category(category_name: str, category_code: str, driver: webdriver.Chrome) -> list:
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\nğŸ”„ {category_name} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘...")
    
    # URL ìƒì„±
    url = f"https://www.musinsa.com/category/{category_code}?{sex}&{sort}"
    driver.get(url)
    time.sleep(3)
    
    # ìŠ¤í¬ë¡¤ì„ ë°˜ë³µí•´ì„œ ìƒí’ˆì´ ë¡œë”©ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
    last_height = driver.execute_script("return document.body.scrollHeight")
    retries = 0
    
    while True:
        # ìŠ¤í¬ë¡¤ ìµœí•˜ë‹¨ìœ¼ë¡œ ì´ë™
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_PAUSE_TIME)
        
        # ìƒˆ ë†’ì´ í™•ì¸
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            retries += 1
            if retries >= 3:
                break
        else:
            retries = 0
            last_height = new_height
        
        # í˜„ì¬ ìƒí’ˆ ê°œìˆ˜ í™•ì¸
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('a.gtm-view-item-list')
        if len(items) >= TARGET_COUNT:
            break
    
    print(f"[INFO] {category_name} ì¹´í…Œê³ ë¦¬ì—ì„œ {len(items)}ê°œì˜ ìƒí’ˆì´ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # íŒŒì‹± ì‹œì‘
    data = []
    collected = 0
    
    for idx, item in enumerate(items):
        if collected >= TARGET_COUNT:
            break
            
        try:
            img_tag = item.find('img')
            if not img_tag:
                continue
                
            # src ì†ì„± ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            image_url = img_tag.get('src', '')
            if not image_url:
                continue
                
            # urlparseì— ë¬¸ìì—´ ì „ë‹¬
            parsed_url = urlparse(str(image_url))
            image_name = f"{category_name}_{collected+1:03d}_{os.path.basename(parsed_url.path)}"
            image_path = os.path.join(SAVE_DIR, image_name)
            
            # ì´ë¯¸ì§€ ì €ì¥
            response = requests.get(str(image_url), stream=True)
            if response.status_code == 200:
                with open(image_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
            
            product = {
                "category": category_name,
                "category_code": category_code,
                "product_id": item.get('data-item-id'),
                "brand_en": item.get('data-item-brand'),
                "brand_kr": item.get('data-brand-id'),
                "price": safe_int(item.get('data-price', 0)),
                "original_price": safe_int(item.get('data-original-price', 0)),
                "discount_rate": safe_int(item.get('data-discount-rate', 0)),
                "product_url": item.get('href'),
                "image_url": image_url,
                "image_path": image_path,
                "product_name": img_tag.get('alt', '')
            }
            data.append(product)
            collected += 1
        except Exception as e:
            print(f"ì—ëŸ¬ ë°œìƒ: {e}")
    
    print(f"âœ… {category_name} ì¹´í…Œê³ ë¦¬ì—ì„œ {len(data)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
    return data

def safe_int(value: Union[str, list, int, None]) -> int:
    """ì•ˆì „í•˜ê²Œ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    if isinstance(value, int):
        return value
    if isinstance(value, str):
        try:
            return int(value)
        except ValueError:
            return 0
    return 0

# Selenium ë“œë¼ì´ë²„ ì‹œì‘
driver = webdriver.Chrome()

# ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
all_data = []
for category_name, category_code in categories.items():
    try:
        category_data = crawl_category(category_name, category_code, driver)
        all_data.extend(category_data)
        print(f"ğŸ“Š {category_name}: {len(category_data)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ {category_name} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

driver.quit()

# ê²°ê³¼ í™•ì¸
print(f"\nğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
print(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ëœ ìƒí’ˆ ìˆ˜: {len(all_data)}ê°œ")

# ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
df_temp = pd.DataFrame(all_data)
category_stats = df_temp['category'].value_counts()
print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
for category, count in category_stats.items():
    print(f"   - {category}: {count}ê°œ")

# DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  CSV íŒŒì¼ë¡œ ì €ì¥
df = pd.DataFrame(all_data)
csv_filename = "musinsa_products_all_categories.csv"
df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
print(f"\nğŸ“Š ë°ì´í„°ê°€ '{csv_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ“‹ DataFrame ì •ë³´:")
print(f"   - í–‰ ìˆ˜: {len(df)}")
print(f"   - ì—´ ìˆ˜: {len(df.columns)}")
print(f"   - ì—´ ì´ë¦„: {list(df.columns)}")
print(f"\nğŸ“ˆ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
print(df.head())
