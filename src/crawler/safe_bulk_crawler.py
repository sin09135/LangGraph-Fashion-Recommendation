import requests
from bs4 import BeautifulSoup
import json
import random
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import re
import os

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36',
]

def setup_driver():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    user_agent = random.choice(USER_AGENTS)
    chrome_options.add_argument(f'--user-agent={user_agent}')
    driver = webdriver.Chrome(options=chrome_options)
    return driver

def extract_product_info(driver, url):
    """ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
    try:
        print(f"ğŸ” {url} ì²˜ë¦¬ ì¤‘...")
        
        # í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        time.sleep(random.uniform(2, 4))
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ë‘˜ëŸ¬ì‹¼ div êµ¬ì¡°ì—ì„œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ (ì°¸ê³ ìš©)
        print("ğŸ” ë‘˜ëŸ¬ì‹¼ div êµ¬ì¡°ì—ì„œ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        main_div = soup.select_one('#root > div:nth-of-type(1) > div:nth-of-type(2) > div')
        
        if main_div:
            all_text = main_div.get_text(strip=True)
            print(f"ğŸ“ ìˆ˜ì§‘ëœ ì „ì²´ í…ìŠ¤íŠ¸: {all_text[:200]}...")
        else:
            print("âŒ ë‘˜ëŸ¬ì‹¼ divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìƒí’ˆëª… ì¶”ì¶œ
        print("ğŸ” ìƒí’ˆëª… ì¶”ì¶œ ì¤‘...")
        product_name = extract_product_name(driver)
        print(f"âœ… ìƒí’ˆëª… ì¶”ì¶œ: {product_name}")
        
        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        print("ğŸ” ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì¤‘...")
        categories = extract_categories(driver)
        print(f"âœ… ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ: {categories}")
        
        # ì—°ê´€íƒœê·¸ ì¶”ì¶œ
        print("ğŸ” ì—°ê´€íƒœê·¸ ì¶”ì¶œ ì¤‘...")
        tags = extract_tags(driver)
        print(f"âœ… í•´ì‹œíƒœê·¸ ì¶”ì¶œ: {tags}")
        
        # ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ì¶œ
        print("ğŸ” ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        size_info = extract_size_info(driver)
        print(f"âœ… ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ì¶œ: {len(size_info)}ê°œ í•­ëª©")
        
        # í›„ê¸° ì •ë³´ ì¶”ì¶œ
        print("ğŸ” í›„ê¸° ì •ë³´ ì¶”ì¶œ ì¤‘...")
        review_info = extract_review_info(driver)
        print(f"âœ… í›„ê¸° ì •ë³´ ì¶”ì¶œ: í‰ì  {review_info.get('rating', 'N/A')}, ê°œìˆ˜ {review_info.get('count', 'N/A')}")
        
        print("âœ… ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ!")
        
        return {
            'url': url,
            'product_name': product_name,
            'categories': categories,
            'tags': tags,
            'size_info': size_info,
            'review_info': review_info
        }
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            'url': url,
            'product_name': 'ì¶”ì¶œ ì‹¤íŒ¨',
            'categories': [],
            'tags': [],
            'size_info': {},
            'review_info': {},
            'error': str(e)
        }

def extract_product_name(driver):
    """ìƒí’ˆëª… ì¶”ì¶œ"""
    try:
        name_selectors = [
            "//div[contains(@class, 'sc-1omefes-0')]//span",
            "//*[@id='root']/div[1]/div[2]/div/div[3]/span",
            "//*[@id='root']/div[1]/div[2]/div/div[4]/span"
        ]
        for selector in name_selectors:
            try:
                element = driver.find_element(By.XPATH, selector)
                name = element.text.strip()
                if name and len(name) > 2:
                    return name
            except:
                continue
        return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"
    except:
        return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"

def extract_categories(driver):
    """ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    try:
        category_selectors = [
            "//div[contains(@class, 'sc-1prswe3-1')]//a",
            "//*[@id='root']/div[1]/div[2]/div/div[2]/span[2]/a",
            "//*[@id='root']/div[1]/div[2]/div/div[2]/span[3]/a",
            "//*[@id='root']/div[1]/div[2]/div/div[3]/span[2]/a",
            "//*[@id='root']/div[1]/div[2]/div/div[3]/span[3]/a"
        ]
        categories = []
        for selector in category_selectors:
            try:
                elements = driver.find_elements(By.XPATH, selector)
                for element in elements:
                    category = element.text.strip()
                    if category and category not in categories:
                        categories.append(category)
            except:
                continue
        return categories
    except:
        return []

def extract_tags(driver):
    """ì—°ê´€íƒœê·¸ ì¶”ì¶œ"""
    try:
        tag_selectors = [
            "//div[contains(@class, 'sc-1eb70kd-0')]",
            "//*[@id='root']/div[1]/div[2]/div/div[16]/ul",
            "//*[@id='root']/div[1]/div[2]/div/div[18]/ul"
        ]
        for selector in tag_selectors:
            try:
                element = driver.find_element(By.XPATH, selector)
                tag_text = element.text
                print(f"âœ… ì—°ê´€íƒœê·¸ í…ìŠ¤íŠ¸: {tag_text[:100]}...")
                tags = []
                if '#' in tag_text:
                    words = tag_text.split()
                    for word in words:
                        if word.startswith('#'):
                            tags.append(word)
                return tags
            except:
                continue
        return []
    except:
        return []

def extract_size_info(driver):
    """ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ì¶œ"""
    try:
        size_button_selectors = [
            "//button[@data-button-name='ì‚¬ì´ì¦ˆíƒ­í´ë¦­']",
            "//*[@id='root']/div[1]/div[1]/div[2]/div/button[2]"
        ]
        for selector in size_button_selectors:
            try:
                size_button = driver.find_element(By.XPATH, selector)
                driver.execute_script("arguments[0].click();", size_button)
                time.sleep(2)
                break
            except:
                continue
        size_data = {}
        try:
            table_selectors = [
                "//table[contains(@class, 'sc-1jg999i-9')]",
                "//*[@id='root']/div[1]/div[1]/div[4]/div/div[2]/div/table",
                "//*[@id='root']/div[1]/div[1]/div[4]/div/div[2]/div[2]/div/table"
            ]
            for selector in table_selectors:
                try:
                    table = driver.find_element(By.XPATH, selector)
                    rows = table.find_elements(By.TAG_NAME, "tr")
                    if len(rows) > 1:
                        headers = []
                        header_cells = rows[0].find_elements(By.TAG_NAME, "th")
                        for cell in header_cells:
                            headers.append(cell.text.strip())
                        size_data['headers'] = headers
                        size_data['rows'] = []
                        for row in rows[1:]:
                            cells = row.find_elements(By.TAG_NAME, "td")
                            row_data = []
                            for cell in cells:
                                row_data.append(cell.text.strip())
                            if row_data:
                                size_data['rows'].append(row_data)
                        break
                except:
                    continue
        except Exception as e:
            size_data['error'] = str(e)
        return size_data
    except Exception as e:
        return {'error': str(e)}

def extract_review_info(driver):
    """í›„ê¸° ì •ë³´ ì¶”ì¶œ"""
    try:
        review_button_selectors = [
            "//button[@data-button-name='ìŠ¤ëƒ…Â·í›„ê¸°íƒ­í´ë¦­']",
            "//*[@id='root']/div[1]/div[1]/div[2]/div/button[4]"
        ]
        for selector in review_button_selectors:
            try:
                review_button = driver.find_element(By.XPATH, selector)
                driver.execute_script("arguments[0].click();", review_button)
                time.sleep(2)
                break
            except:
                continue
        review_data = {}
        try:
            rating_selectors = [
                "//div[contains(@class, 'GoodsReviewTitleSection__TitleContainer')]",
                "//*[@id='root']/div[1]/div[1]/div[6]/div[2]/div/div/div[4]/div[7]/div[1]/div[3]/div[3]/div/div[2]"
            ]
            for selector in rating_selectors:
                try:
                    rating_element = driver.find_element(By.XPATH, selector)
                    rating_text = rating_element.text
                    rating_match = re.search(r'(\d+\.\d+)', rating_text)
                    if rating_match:
                        review_data['rating'] = rating_match.group(1)
                    count_match = re.search(r'\((\d+(?:,\d+)*)\)', rating_text)
                    if count_match:
                        count_str = count_match.group(1).replace(',', '')
                        review_data['count'] = int(count_str)
                    break
                except:
                    continue
        except Exception as e:
            review_data['rating_error'] = str(e)
        try:
            review_selectors = [
                "//div[contains(@class, 'GoodsReviewStaticList__Container')]//div[contains(@class, 'review-list-item__Container')]",
                "//*[@id='root']/div[1]/div[1]/div[6]/div[2]/div/div/div[4]/div[7]/div"
            ]
            reviews = []
            for selector in review_selectors:
                try:
                    review_elements = driver.find_elements(By.XPATH, selector)
                    for i, element in enumerate(review_elements[:5]):
                        try:
                            review_info = {'index': i + 1}
                            content_selectors = [
                                ".//span[contains(@class, 'text-body_13px_reg') and contains(@class, 'text-black')]",
                                ".//div[contains(@class, 'ExpandableContent__ContentContainer')]//span[contains(@class, 'text-body_13px_reg')]",
                                ".//div[contains(@class, 'ExpandableContent__ContentContainer')]//span"
                            ]
                            content = ""
                            for content_selector in content_selectors:
                                try:
                                    content_elements = element.find_elements(By.XPATH, content_selector)
                                    for content_element in content_elements:
                                        text = content_element.text.strip()
                                        if text and len(text) > 10:
                                            content = text
                                            break
                                    if content:
                                        break
                                except:
                                    continue
                            if content:
                                review_info['content'] = content[:300] + "..." if len(content) > 300 else content
                            like_selectors = [
                                ".//div[contains(@class, 'LikeButton__Container')]//span[contains(@class, 'text-body_13px_reg')]",
                                ".//div[contains(@class, 'InteractionSection__Container')]//span[contains(@class, 'text-body_13px_reg')]"
                            ]
                            like_count = ""
                            for like_selector in like_selectors:
                                try:
                                    like_elements = element.find_elements(By.XPATH, like_selector)
                                    for like_element in like_elements:
                                        text = like_element.text.strip()
                                        if text.isdigit():
                                            like_count = text
                                            break
                                    if like_count:
                                        break
                                except:
                                    continue
                            if like_count:
                                review_info['likes'] = int(like_count)
                            else:
                                review_info['likes'] = 0
                            comment_selectors = [
                                ".//a[contains(@class, 'CommentButton__Container')]//span[contains(@class, 'text-body_13px_reg')]",
                                ".//div[contains(@class, 'InteractionSection__Container')]//a//span[contains(@class, 'text-body_13px_reg')]"
                            ]
                            comment_count = ""
                            for comment_selector in comment_selectors:
                                try:
                                    comment_elements = element.find_elements(By.XPATH, comment_selector)
                                    for comment_element in comment_elements:
                                        text = comment_element.text.strip()
                                        if text.isdigit():
                                            comment_count = text
                                            break
                                    if comment_count:
                                        break
                                except:
                                    continue
                            if comment_count:
                                review_info['comments'] = int(comment_count)
                            else:
                                review_info['comments'] = 0
                            user_selectors = [
                                ".//span[contains(@class, 'UserProfileSection__Nickname')]",
                                ".//div[contains(@class, 'UserProfileSection__Info')]//span[contains(@class, 'text-body_13px_med')]"
                            ]
                            user_name = ""
                            for user_selector in user_selectors:
                                try:
                                    user_element = element.find_element(By.XPATH, user_selector)
                                    user_name = user_element.text.strip()
                                    if user_name:
                                        break
                                except:
                                    continue
                            if user_name:
                                review_info['user'] = user_name
                            date_selectors = [
                                ".//span[contains(@class, 'UserProfileSection__PurchaseDate')]",
                                ".//span[contains(@class, 'text-body_13px_reg') and contains(@class, 'text-gray-500')]"
                            ]
                            review_date = ""
                            for date_selector in date_selectors:
                                try:
                                    date_elements = element.find_elements(By.XPATH, date_selector)
                                    for date_element in date_elements:
                                        text = date_element.text.strip()
                                        if re.match(r'\d{2}\.\d{2}\.\d{2}|\d{4}\.\d{2}\.\d{2}', text):
                                            review_date = text
                                            break
                                    if review_date:
                                        break
                                except:
                                    continue
                            if review_date:
                                review_info['date'] = review_date
                            purchase_selectors = [
                                ".//div[contains(@class, 'UserInfoGoodsOptionSection')]//span[contains(@class, 'text-body_13px_reg')]"
                            ]
                            purchase_info = ""
                            for purchase_selector in purchase_selectors:
                                try:
                                    purchase_elements = element.find_elements(By.XPATH, purchase_selector)
                                    for purchase_element in purchase_elements:
                                        text = purchase_element.text.strip()
                                        if "êµ¬ë§¤" in text or "Â·" in text:
                                            purchase_info = text
                                            break
                                    if purchase_info:
                                        break
                                except:
                                    continue
                            if purchase_info:
                                review_info['purchase_info'] = purchase_info
                            if review_info.get('content'):
                                reviews.append(review_info)
                        except Exception as e:
                            continue
                    if reviews:
                        break
                except:
                    continue
            review_data['reviews'] = reviews
        except Exception as e:
            review_data['reviews_error'] = str(e)
        return review_data
    except Exception as e:
        return {'error': str(e)}

# --- ë©”ì¸ í•¨ìˆ˜ ---
def main():
    try:
        df = pd.read_csv('musinsa_products_all_categories.csv')
        all_urls = df['product_url'].tolist()
        print(f"ğŸ“Š ì´ {len(all_urls)}ê°œ ìƒí’ˆ URL ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return

    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸
    results = []
    processed_urls = set()
    failed_urls_set = set()
    
    checkpoint_file = 'safe_bulk_crawler_checkpoint.json'
    failed_file = 'safe_bulk_crawler_failed_urls.txt'
    
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # ì„±ê³µí•œ URLê³¼ ì‹¤íŒ¨í•œ URL ë¶„ë¦¬
            for item in results:
                if 'url' in item:
                    processed_urls.add(item['url'])
                    if item.get('product_name') == 'ì¶”ì¶œ ì‹¤íŒ¨':
                        failed_urls_set.add(item['url'])
            
            print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë°œê²¬: {len(results)}ê°œ ìƒí’ˆ ì´ë¯¸ ì²˜ë¦¬ë¨")
            print(f"âŒ ì‹¤íŒ¨í•œ URL: {len(failed_urls_set)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    # ì²˜ë¦¬í•  URLë“¤ (ìƒˆë¡œìš´ URL + ì‹¤íŒ¨í•œ URLë“¤)
    urls_to_process = []
    
    # ìƒˆë¡œìš´ URLë“¤ ì¶”ê°€
    new_urls = [url for url in all_urls if url not in processed_urls]
    urls_to_process.extend(new_urls)
    
    # ì‹¤íŒ¨í•œ URLë“¤ ë‹¤ì‹œ ì¶”ê°€
    urls_to_process.extend(list(failed_urls_set))
    
    print(f"ğŸ”„ ì²˜ë¦¬í•  ìƒí’ˆ: {len(urls_to_process)}ê°œ")
    print(f"  - ìƒˆë¡œìš´ URL: {len(new_urls)}ê°œ")
    print(f"  - ì¬ì‹œë„ URL: {len(failed_urls_set)}ê°œ")
    
    if not urls_to_process:
        print("âœ… ëª¨ë“  ìƒí’ˆì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return

    driver = setup_driver()
    failed_urls = []
    checkpoint_interval = 5  # 5ê°œë§ˆë‹¤ ì €ì¥ (ë” ì•ˆì „í•˜ê²Œ)

    successful_count = len([item for item in results if item.get('product_name') != 'ì¶”ì¶œ ì‹¤íŒ¨'])
    failed_count = len([item for item in results if item.get('product_name') == 'ì¶”ì¶œ ì‹¤íŒ¨'])

    try:
        for i, url in enumerate(urls_to_process, len(results) + 1):
            print("=" * 50)
            print(f"ì§„í–‰ë¥ : {i}/{len(urls_to_process)} - {url}")
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸
            existing_result = None
            for result_item in results:
                if result_item.get('url') == url:
                    existing_result = result_item
                    break
            
            # ì‹¤íŒ¨í•œ URLì¸ ê²½ìš° ê¸°ì¡´ ê²°ê³¼ ì œê±°
            if existing_result and existing_result.get('product_name') == 'ì¶”ì¶œ ì‹¤íŒ¨':
                results.remove(existing_result)
                print(f"  ğŸ”„ ì‹¤íŒ¨í•œ URL ì¬ì‹œë„: {url}")
            
            try:
                result = extract_product_info(driver, url)
                results.append(result)
                
                if result['product_name'] != 'ì¶”ì¶œ ì‹¤íŒ¨':
                    successful_count += 1
                    print(f"  âœ“ ì„±ê³µ: {result['product_name']}")
                else:
                    failed_count += 1
                    failed_urls.append(url)
                    print(f"  âœ— ì‹¤íŒ¨: ì¶”ì¶œ ì‹¤íŒ¨")
                
            except Exception as e:
                print(f"  âœ— ì˜¤ë¥˜: {e}")
                # ì„¸ì…˜ ì˜¤ë¥˜ì¸ ê²½ìš° ë“œë¼ì´ë²„ ì¬ì‹œì‘
                if "invalid session id" in str(e).lower():
                    print("  ğŸ”„ ì„¸ì…˜ ì˜¤ë¥˜ ë°œìƒ, ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì¤‘...")
                    driver.quit()
                    driver = setup_driver()
                
                results.append({
                    'url': url,
                    'product_name': 'ì¶”ì¶œ ì‹¤íŒ¨',
                    'categories': [],
                    'tags': [],
                    'size_info': {},
                    'review_info': {},
                    'error': str(e)
                })
                failed_urls.append(url)
                failed_count += 1
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (5ê°œë§ˆë‹¤)
            if i % checkpoint_interval == 0 or i == len(urls_to_process):
                with open(checkpoint_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ {i}ê°œê¹Œì§€ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ!")
                if failed_urls:
                    with open(failed_file, 'w', encoding='utf-8') as f:
                        for fail_url in failed_urls:
                            f.write(fail_url + '\n')
            
            # 5~15ì´ˆ ëœë¤ ëŒ€ê¸°
            if i < len(urls_to_process):
                wait_time = random.uniform(5, 15)
                print(f"â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(wait_time)
    
    finally:
        driver.quit()
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    with open('safe_bulk_crawler_final.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ì„±ê³µí•œ ë°ì´í„°ë§Œ ë³„ë„ ì €ì¥
    successful_data = [item for item in results if item.get('product_name') != 'ì¶”ì¶œ ì‹¤íŒ¨']
    with open('safe_bulk_crawler_successful.json', 'w', encoding='utf-8') as f:
        json.dump(successful_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ì„±ê³µ: {successful_count}ê°œ")
    print(f"ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"ì´ ì²˜ë¦¬: {len(all_urls)}ê°œ")
    print(f"ì„±ê³µë¥ : {successful_count/len(all_urls)*100:.1f}%")
    print(f"ê²°ê³¼ê°€ '{checkpoint_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ì„±ê³µí•œ ë°ì´í„° {len(successful_data)}ê°œë¥¼ 'safe_bulk_crawler_successful.json'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    if failed_urls:
        print(f"ì‹¤íŒ¨í•œ URLì€ '{failed_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main() 