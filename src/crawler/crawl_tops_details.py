import requests
from bs4 import BeautifulSoup
import json
import random
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import re
import os

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36',
]

def setup_driver():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    user_agent = random.choice(USER_AGENTS)
    chrome_options.add_argument(f'--user-agent={user_agent}')
    driver = webdriver.Chrome(options=chrome_options)
    return driver

def extract_product_info(driver, url):
    """ÏÉÅÌíà Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
    try:
        print(f"üîç {url} Ï≤òÎ¶¨ Ï§ë...")
        
        # ÌéòÏù¥ÏßÄ Î°úÎìú
        driver.get(url)
        time.sleep(random.uniform(2, 4))
        
        # BeautifulSoupÏúºÎ°ú ÌååÏã±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ÎëòÎü¨Ïãº div Íµ¨Ï°∞ÏóêÏÑú Í∏∞Î≥∏ Ï†ïÎ≥¥ ÏàòÏßë (Ï∞∏Í≥†Ïö©)
        print("üîç ÎëòÎü¨Ïãº div Íµ¨Ï°∞ÏóêÏÑú Ï†ïÎ≥¥ ÏàòÏßë Ï§ë...")
        main_div = soup.select_one('#root > div:nth-of-type(1) > div:nth-of-type(2) > div')
        
        if main_div:
            all_text = main_div.get_text(strip=True)
            print(f"üìù ÏàòÏßëÎêú Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏: {all_text[:200]}...")
        else:
            print("‚ùå ÎëòÎü¨Ïãº divÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
        
        # ÏÉÅÌíàÎ™Ö Ï∂îÏ∂ú
        print("üîç ÏÉÅÌíàÎ™Ö Ï∂îÏ∂ú Ï§ë...")
        product_name = extract_product_name(driver)
        print(f"‚úÖ ÏÉÅÌíàÎ™Ö Ï∂îÏ∂ú: {product_name}")
        
        # Ïπ¥ÌÖåÍ≥†Î¶¨ Ï∂îÏ∂ú
        print("üîç Ïπ¥ÌÖåÍ≥†Î¶¨ Ï∂îÏ∂ú Ï§ë...")
        categories = extract_categories(driver)
        print(f"‚úÖ Ïπ¥ÌÖåÍ≥†Î¶¨ Ï∂îÏ∂ú: {categories}")
        
        # Ïó∞Í¥ÄÌÉúÍ∑∏ Ï∂îÏ∂ú
        print("üîç Ïó∞Í¥ÄÌÉúÍ∑∏ Ï∂îÏ∂ú Ï§ë...")
        tags = extract_tags(driver)
        print(f"‚úÖ Ìï¥ÏãúÌÉúÍ∑∏ Ï∂îÏ∂ú: {tags}")
        
        # ÏÇ¨Ïù¥Ï¶à Ï†ïÎ≥¥ Ï∂îÏ∂ú
        print("üîç ÏÇ¨Ïù¥Ï¶à Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë...")
        size_info = extract_size_info(driver)
        print(f"‚úÖ ÏÇ¨Ïù¥Ï¶à Ï†ïÎ≥¥ Ï∂îÏ∂ú: {len(size_info)}Í∞ú Ìï≠Î™©")
        
        # Ìïè/Í≥ÑÏ†àÍ∞ê Ï†ïÎ≥¥ Ï∂îÏ∂ú
        print("üîç Ìïè/Í≥ÑÏ†àÍ∞ê Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë...")
        fit_season_info = extract_fit_season_info(driver)
        print(f"‚úÖ Ìïè/Í≥ÑÏ†àÍ∞ê Ï†ïÎ≥¥ Ï∂îÏ∂ú: {len(fit_season_info)}Í∞ú Ìï≠Î™©")
        
        # ÌõÑÍ∏∞ Ï†ïÎ≥¥ Ï∂îÏ∂ú
        print("üîç ÌõÑÍ∏∞ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë...")
        review_info = extract_review_info(driver)
        print(f"‚úÖ ÌõÑÍ∏∞ Ï†ïÎ≥¥ Ï∂îÏ∂ú: ÌèâÏ†ê {review_info.get('rating', 'N/A')}, Í∞úÏàò {review_info.get('count', 'N/A')}")
        
        print("‚úÖ ÏÉÅÌíà Ï†ïÎ≥¥ Ï∂îÏ∂ú ÏôÑÎ£å!")
        
        return {
            'url': url,
            'product_name': product_name,
            'categories': categories,
            'tags': tags,
            'size_info': size_info,
            'fit_season_info': fit_season_info,
            'review_info': review_info
        }
        
    except Exception as e:
        print(f"‚ùå Ïò§Î•ò Î∞úÏÉù: {str(e)}")
        return {
            'url': url,
            'product_name': 'Ï∂îÏ∂ú Ïã§Ìå®',
            'categories': [],
            'tags': [],
            'size_info': {},
            'fit_season_info': {},
            'review_info': {},
            'error': str(e)
        }

def extract_product_name(driver):
    """ÏÉÅÌíàÎ™Ö Ï∂îÏ∂ú"""
    try:
        name_selectors = [
            "//div[contains(@class, 'sc-1omefes-0')]//span",
            "//*[@id='root']/div[1]/div[2]/div/div[3]/span",
            "//*[@id='root']/div[1]/div[2]/div/div[4]/span"
        ]
        for selector in name_selectors:
            try:
                element = driver.find_element(By.XPATH, selector)
                name = element.text.strip()
                if name and len(name) > 2:
                    return name
            except:
                continue
        return "ÏÉÅÌíàÎ™Ö Ï∂îÏ∂ú Ïã§Ìå®"
    except:
        return "ÏÉÅÌíàÎ™Ö Ï∂îÏ∂ú Ïã§Ìå®"

def extract_categories(driver):
    """Ïπ¥ÌÖåÍ≥†Î¶¨ Ï∂îÏ∂ú"""
    try:
        category_selectors = [
            "//div[contains(@class, 'sc-1prswe3-1')]//a",
            "//*[@id='root']/div[1]/div[2]/div/div[2]/span[2]/a",
            "//*[@id='root']/div[1]/div[2]/div/div[2]/span[3]/a",
            "//*[@id='root']/div[1]/div[2]/div/div[3]/span[2]/a",
            "//*[@id='root']/div[1]/div[2]/div/div[3]/span[3]/a"
        ]
        categories = []
        for selector in category_selectors:
            try:
                elements = driver.find_elements(By.XPATH, selector)
                for element in elements:
                    category = element.text.strip()
                    if category and category not in categories:
                        categories.append(category)
            except:
                continue
        return categories
    except:
        return []

def extract_tags(driver):
    """Ïó∞Í¥ÄÌÉúÍ∑∏ Ï∂îÏ∂ú"""
    try:
        tag_selectors = [
            "//div[contains(@class, 'sc-1eb70kd-0')]",
            "//*[@id='root']/div[1]/div[2]/div/div[16]/ul",
            "//*[@id='root']/div[1]/div[2]/div/div[18]/ul"
        ]
        for selector in tag_selectors:
            try:
                element = driver.find_element(By.XPATH, selector)
                tag_text = element.text
                print(f"‚úÖ Ïó∞Í¥ÄÌÉúÍ∑∏ ÌÖçÏä§Ìä∏: {tag_text[:100]}...")
                tags = []
                if '#' in tag_text:
                    words = tag_text.split()
                    for word in words:
                        if word.startswith('#'):
                            tags.append(word)
                return tags
            except:
                continue
        return []
    except:
        return []

def extract_size_info(driver):
    """ÏÇ¨Ïù¥Ï¶à Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
    try:
        size_button_selectors = [
            "//button[@data-button-name='ÏÇ¨Ïù¥Ï¶àÌÉ≠ÌÅ¥Î¶≠']",
            "//*[@id='root']/div[1]/div[1]/div[2]/div/button[2]"
        ]
        for selector in size_button_selectors:
            try:
                size_button = driver.find_element(By.XPATH, selector)
                driver.execute_script("arguments[0].click();", size_button)
                time.sleep(2)
                break
            except:
                continue
        size_data = {}
        try:
            table_selectors = [
                "//table[contains(@class, 'sc-1jg999i-9')]",
                "//*[@id='root']/div[1]/div[1]/div[4]/div/div[2]/div/table",
                "//*[@id='root']/div[1]/div[1]/div[4]/div/div[2]/div[2]/div/table"
            ]
            for selector in table_selectors:
                try:
                    table = driver.find_element(By.XPATH, selector)
                    rows = table.find_elements(By.TAG_NAME, "tr")
                    if len(rows) > 1:
                        headers = []
                        header_cells = rows[0].find_elements(By.TAG_NAME, "th")
                        for cell in header_cells:
                            headers.append(cell.text.strip())
                        size_data['headers'] = headers
                        size_data['rows'] = []
                        for row in rows[1:]:
                            cells = row.find_elements(By.TAG_NAME, "td")
                            row_data = []
                            for cell in cells:
                                row_data.append(cell.text.strip())
                            if row_data:
                                size_data['rows'].append(row_data)
                        break
                except:
                    continue
        except Exception as e:
            size_data['error'] = str(e)
        return size_data
    except Exception as e:
        return {'error': str(e)}

def extract_fit_season_info(driver):
    """Ìïè/Í≥ÑÏ†àÍ∞ê Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
    try:
        fit_data = {}
        try:
            # Ìïè/Í≥ÑÏ†àÍ∞ê div Ï∞æÍ∏∞
            fit_selectors = [
                "//div[contains(@class, 'sc-36xiah-2')]",
                "//div[contains(@class, 'fvqqbN')]"
            ]
            fit_div = None
            for selector in fit_selectors:
                try:
                    fit_div = driver.find_element(By.XPATH, selector)
                    break
                except:
                    continue
            
            if fit_div:
                # Ìó§Îçî Ï∂îÏ∂ú (Ìïè, Ï¥âÍ∞ê, Ïã†Ï∂ïÏÑ±, ÎπÑÏπ®, ÎëêÍªò, Í≥ÑÏ†à)
                header_selectors = [
                    ".//ul[contains(@class, 'sc-36xiah-3')]//li",
                    ".//ul[contains(@class, 'iZBEnN')]//li"
                ]
                headers = []
                for selector in header_selectors:
                    try:
                        header_elements = fit_div.find_elements(By.XPATH, selector)
                        for element in header_elements:
                            header_text = element.text.strip()
                            if header_text:
                                headers.append(header_text)
                        if headers:
                            break
                    except:
                        continue
                fit_data['headers'] = headers
                
                # ÌÖåÏù¥Î∏î Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
                table_selectors = [
                    ".//table[contains(@class, 'sc-36xiah-6')]",
                    ".//table[contains(@class, 'jizuRz')]"
                ]
                for selector in table_selectors:
                    try:
                        table = fit_div.find_element(By.XPATH, selector)
                        rows = table.find_elements(By.TAG_NAME, "tr")
                        fit_data['rows'] = []
                        for row in rows:
                            cells = row.find_elements(By.TAG_NAME, "td")
                            row_data = []
                            for cell in cells:
                                cell_text = cell.text.strip()
                                if cell_text:
                                    # Í∞ïÏ°∞Îêú ÏÖÄ(eviTcu ÌÅ¥ÎûòÏä§)ÏùÄ ÏÑ†ÌÉùÎêú Í∞í
                                    if "eviTcu" in cell.get_attribute("class"):
                                        row_data.append(f"‚úì {cell_text}")
                                    else:
                                        row_data.append(cell_text)
                            if row_data:
                                fit_data['rows'].append(row_data)
                        break
                    except:
                        continue
            else:
                fit_data['error'] = "Ìïè/Í≥ÑÏ†àÍ∞ê Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"
        except Exception as e:
            fit_data['error'] = str(e)
        return fit_data
    except Exception as e:
        return {'error': str(e)}

def extract_review_info(driver):
    """ÌõÑÍ∏∞ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
    try:
        review_button_selectors = [
            "//button[@data-button-name='Ïä§ÎÉÖ¬∑ÌõÑÍ∏∞ÌÉ≠ÌÅ¥Î¶≠']",
            "//*[@id='root']/div[1]/div[1]/div[2]/div/button[4]"
        ]
        for selector in review_button_selectors:
            try:
                review_button = driver.find_element(By.XPATH, selector)
                driver.execute_script("arguments[0].click();", review_button)
                time.sleep(2)
                break
            except:
                continue
        review_data = {}
        try:
            rating_selectors = [
                "//div[contains(@class, 'GoodsReviewTitleSection__TitleContainer')]",
                "//*[@id='root']/div[1]/div[1]/div[6]/div[2]/div/div/div[4]/div[7]/div[1]/div[3]/div[3]/div/div[2]"
            ]
            for selector in rating_selectors:
                try:
                    rating_element = driver.find_element(By.XPATH, selector)
                    rating_text = rating_element.text
                    rating_match = re.search(r'(\d+\.\d+)', rating_text)
                    if rating_match:
                        review_data['rating'] = rating_match.group(1)
                    count_match = re.search(r'\((\d+(?:,\d+)*)\)', rating_text)
                    if count_match:
                        count_str = count_match.group(1).replace(',', '')
                        review_data['count'] = int(count_str)
                    break
                except:
                    continue
        except Exception as e:
            review_data['rating_error'] = str(e)
        try:
            review_selectors = [
                "//div[contains(@class, 'GoodsReviewStaticList__Container')]//div[contains(@class, 'review-list-item__Container')]",
                "//*[@id='root']/div[1]/div[1]/div[6]/div[2]/div/div/div[4]/div[7]/div"
            ]
            reviews = []
            for selector in review_selectors:
                try:
                    review_elements = driver.find_elements(By.XPATH, selector)
                    for i, element in enumerate(review_elements[:5]):
                        try:
                            review_info = {'index': i + 1}
                            content_selectors = [
                                ".//span[contains(@class, 'text-body_13px_reg') and contains(@class, 'text-black')]",
                                ".//div[contains(@class, 'ExpandableContent__ContentContainer')]//span[contains(@class, 'text-body_13px_reg')]",
                                ".//div[contains(@class, 'ExpandableContent__ContentContainer')]//span"
                            ]
                            content = ""
                            for content_selector in content_selectors:
                                try:
                                    content_elements = element.find_elements(By.XPATH, content_selector)
                                    for content_element in content_elements:
                                        text = content_element.text.strip()
                                        if text and len(text) > 10:
                                            content = text
                                            break
                                    if content:
                                        break
                                except:
                                    continue
                            if content:
                                review_info['content'] = content[:300] + "..." if len(content) > 300 else content
                            like_selectors = [
                                ".//div[contains(@class, 'LikeButton__Container')]//span[contains(@class, 'text-body_13px_reg')]",
                                ".//div[contains(@class, 'InteractionSection__Container')]//span[contains(@class, 'text-body_13px_reg')]"
                            ]
                            like_count = ""
                            for like_selector in like_selectors:
                                try:
                                    like_elements = element.find_elements(By.XPATH, like_selector)
                                    for like_element in like_elements:
                                        text = like_element.text.strip()
                                        if text.isdigit():
                                            like_count = text
                                            break
                                    if like_count:
                                        break
                                except:
                                    continue
                            if like_count:
                                review_info['likes'] = int(like_count)
                            else:
                                review_info['likes'] = 0
                            comment_selectors = [
                                ".//a[contains(@class, 'CommentButton__Container')]//span[contains(@class, 'text-body_13px_reg')]",
                                ".//div[contains(@class, 'InteractionSection__Container')]//a//span[contains(@class, 'text-body_13px_reg')]"
                            ]
                            comment_count = ""
                            for comment_selector in comment_selectors:
                                try:
                                    comment_elements = element.find_elements(By.XPATH, comment_selector)
                                    for comment_element in comment_elements:
                                        text = comment_element.text.strip()
                                        if text.isdigit():
                                            comment_count = text
                                            break
                                    if comment_count:
                                        break
                                except:
                                    continue
                            if comment_count:
                                review_info['comments'] = int(comment_count)
                            else:
                                review_info['comments'] = 0
                            user_selectors = [
                                ".//span[contains(@class, 'UserProfileSection__Nickname')]",
                                ".//div[contains(@class, 'UserProfileSection__Info')]//span[contains(@class, 'text-body_13px_med')]"
                            ]
                            user_name = ""
                            for user_selector in user_selectors:
                                try:
                                    user_element = element.find_element(By.XPATH, user_selector)
                                    user_name = user_element.text.strip()
                                    if user_name:
                                        break
                                except:
                                    continue
                            if user_name:
                                review_info['user'] = user_name
                            date_selectors = [
                                ".//span[contains(@class, 'UserProfileSection__PurchaseDate')]",
                                ".//span[contains(@class, 'text-body_13px_reg') and contains(@class, 'text-gray-500')]"
                            ]
                            review_date = ""
                            for date_selector in date_selectors:
                                try:
                                    date_elements = element.find_elements(By.XPATH, date_selector)
                                    for date_element in date_elements:
                                        text = date_element.text.strip()
                                        if re.match(r'\d{2}\.\d{2}\.\d{2}|\d{4}\.\d{2}\.\d{2}', text):
                                            review_date = text
                                            break
                                    if review_date:
                                        break
                                except:
                                    continue
                            if review_date:
                                review_info['date'] = review_date
                            purchase_selectors = [
                                ".//div[contains(@class, 'UserInfoGoodsOptionSection')]//span[contains(@class, 'text-body_13px_reg')]"
                            ]
                            purchase_info = ""
                            for purchase_selector in purchase_selectors:
                                try:
                                    purchase_elements = element.find_elements(By.XPATH, purchase_selector)
                                    for purchase_element in purchase_elements:
                                        text = purchase_element.text.strip()
                                        if "Íµ¨Îß§" in text or "¬∑" in text:
                                            purchase_info = text
                                            break
                                    if purchase_info:
                                        break
                                except:
                                    continue
                            if purchase_info:
                                review_info['purchase_info'] = purchase_info
                            if review_info.get('content'):
                                reviews.append(review_info)
                        except Exception as e:
                            continue
                    if reviews:
                        break
                except:
                    continue
            review_data['reviews'] = reviews
        except Exception as e:
            review_data['reviews_error'] = str(e)
        return review_data
    except Exception as e:
        return {'error': str(e)}

def crawl_tops_details():
    """ÏÉÅÏùò Ïπ¥ÌÖåÍ≥†Î¶¨ ÏÉÅÌíàÎì§Ïùò ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÌÅ¨Î°§ÎßÅ"""
    
    # CSV ÌååÏùºÏóêÏÑú ÏÉÅÏùò Ïπ¥ÌÖåÍ≥†Î¶¨ URLÎì§ Ï∂îÏ∂ú
    df = pd.read_csv('musinsa_products_all_categories.csv')
    tops_df = df[df['category'] == 'ÏÉÅÏùò']
    all_tops_urls = tops_df['product_url'].tolist()
    
    print(f"üìä ÏÉÅÏùò Ïπ¥ÌÖåÍ≥†Î¶¨ ÏÉÅÌíà Ïàò: {len(all_tops_urls)}Í∞ú")
    
    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº ÌôïÏù∏
    results = []
    processed_urls = set()
    failed_urls = set()
    
    if os.path.exists('tops_details_checkpoint.json'):
        try:
            with open('tops_details_checkpoint.json', 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # ÏÑ±Í≥µÌïú URLÍ≥º Ïã§Ìå®Ìïú URL Î∂ÑÎ¶¨
            for item in results:
                if 'url' in item:
                    processed_urls.add(item['url'])
                    if item.get('product_name') == 'Ï∂îÏ∂ú Ïã§Ìå®':
                        failed_urls.add(item['url'])
            
            print(f"üìÇ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº Î∞úÍ≤¨: {len(results)}Í∞ú ÏÉÅÌíà Ïù¥ÎØ∏ Ï≤òÎ¶¨Îê®")
            print(f"‚ùå Ïã§Ìå®Ìïú URL: {len(failed_urls)}Í∞ú")
        except Exception as e:
            print(f"‚ö†Ô∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùº ÏùΩÍ∏∞ Ïã§Ìå®: {e}")
    
    # Ï≤òÎ¶¨Ìï† URLÎì§ (ÏÉàÎ°úÏö¥ URL + Ïã§Ìå®Ìïú URLÎì§)
    urls_to_process = []
    
    # ÏÉàÎ°úÏö¥ URLÎì§ Ï∂îÍ∞Ä
    new_urls = [url for url in all_tops_urls if url not in processed_urls]
    urls_to_process.extend(new_urls)
    
    # Ïã§Ìå®Ìïú URLÎì§ Îã§Ïãú Ï∂îÍ∞Ä
    urls_to_process.extend(list(failed_urls))
    
    print(f"üîÑ Ï≤òÎ¶¨Ìï† ÏÉÅÌíà: {len(urls_to_process)}Í∞ú")
    print(f"  - ÏÉàÎ°úÏö¥ URL: {len(new_urls)}Í∞ú")
    print(f"  - Ïû¨ÏãúÎèÑ URL: {len(failed_urls)}Í∞ú")
    
    if not urls_to_process:
        print("‚úÖ Î™®Îì† ÏÉÅÌíàÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÎêòÏóàÏäµÎãàÎã§!")
        return
    
    driver = setup_driver()
    successful_count = len([item for item in results if item.get('product_name') != 'Ï∂îÏ∂ú Ïã§Ìå®'])
    failed_count = len([item for item in results if item.get('product_name') == 'Ï∂îÏ∂ú Ïã§Ìå®'])
    
    try:
        for i, url in enumerate(urls_to_process, len(results) + 1):
            print(f"ÏßÑÌñâÎ•†: {i}/{len(urls_to_process)} - {url}")
            
            # Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú URLÏù∏ÏßÄ ÌôïÏù∏
            existing_result = None
            for result_item in results:
                if result_item.get('url') == url:
                    existing_result = result_item
                    break
            
            # Ïã§Ìå®Ìïú URLÏù∏ Í≤ΩÏö∞ Í∏∞Ï°¥ Í≤∞Í≥º Ï†úÍ±∞
            if existing_result and existing_result.get('product_name') == 'Ï∂îÏ∂ú Ïã§Ìå®':
                results.remove(existing_result)
                print(f"  üîÑ Ïã§Ìå®Ìïú URL Ïû¨ÏãúÎèÑ: {url}")
            
            try:
                # ÏÉÅÌíà Ï†ïÎ≥¥ Ï∂îÏ∂ú
                product_data = extract_product_info(driver, url)
                results.append(product_data)
                
                if product_data['product_name'] != "Ï∂îÏ∂ú Ïã§Ìå®":
                    successful_count += 1
                    print(f"  ‚úì ÏÑ±Í≥µ: {product_data['product_name']}")
                else:
                    failed_count += 1
                    print(f"  ‚úó Ïã§Ìå®: Ï∂îÏ∂ú Ïã§Ìå®")
                
                # 10Í∞úÎßàÎã§ Ï§ëÍ∞Ñ Ï†ÄÏû•
                if i % 10 == 0:
                    with open('tops_details_checkpoint.json', 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    print(f"  Ï§ëÍ∞Ñ Ï†ÄÏû• ÏôÑÎ£å ({i}Í∞ú Ï≤òÎ¶¨)")
                
            except Exception as e:
                failed_count += 1
                print(f"  ‚úó Ïò§Î•ò: {e}")
                # ÏÑ∏ÏÖò Ïò§Î•òÏù∏ Í≤ΩÏö∞ ÎìúÎùºÏù¥Î≤Ñ Ïû¨ÏãúÏûë
                if "invalid session id" in str(e).lower():
                    print("  üîÑ ÏÑ∏ÏÖò Ïò§Î•ò Î∞úÏÉù, ÎìúÎùºÏù¥Î≤Ñ Ïû¨ÏãúÏûë Ï§ë...")
                    driver.quit()
                    driver = setup_driver()
                
                results.append({
                    "url": url,
                    "product_name": "Ï∂îÏ∂ú Ïã§Ìå®",
                    "categories": [],
                    "tags": [],
                    "size_info": {},
                    "fit_season_info": {},
                    "review_info": {},
                    "error": str(e)
                })
            
            # ÏöîÏ≤≠ Í∞Ñ ÎûúÎç§ ÎåÄÍ∏∞ (5-15Ï¥à)
            if i < len(urls_to_process):
                wait_time = random.uniform(5, 15)
                print(f"  ÎåÄÍ∏∞ Ï§ë... ({wait_time:.1f}Ï¥à)")
                time.sleep(wait_time)
    
    finally:
        driver.quit()
    
    # ÏµúÏ¢Ö Ï†ÄÏû•
    with open('tops_details_final.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüéâ ÏÉÅÏùò ÏÉÅÏÑ∏ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å!")
    print(f"ÏÑ±Í≥µ: {successful_count}Í∞ú")
    print(f"Ïã§Ìå®: {failed_count}Í∞ú")
    print(f"Ï¥ù Ï≤òÎ¶¨: {len(all_tops_urls)}Í∞ú")
    print(f"ÏÑ±Í≥µÎ•†: {successful_count/len(all_tops_urls)*100:.1f}%")
    
    # ÏÑ±Í≥µÌïú Îç∞Ïù¥ÌÑ∞Îßå Î≥ÑÎèÑ Ï†ÄÏû•
    successful_data = [item for item in results if item['product_name'] != 'Ï∂îÏ∂ú Ïã§Ìå®']
    with open('tops_details_successful.json', 'w', encoding='utf-8') as f:
        json.dump(successful_data, f, ensure_ascii=False, indent=2)
    
    print(f"ÏÑ±Í≥µÌïú Îç∞Ïù¥ÌÑ∞ {len(successful_data)}Í∞úÎ•º 'tops_details_successful.json'Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.")

if __name__ == "__main__":
    crawl_tops_details() 